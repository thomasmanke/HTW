{"cells":[{"cell_type":"markdown","metadata":{},"source":["---\n","title: \"03: Image Classification and Interpretation\"\n","author: \"Thomas Manke\"\n","date:  \"2024-01-05\"\n","categories:\n","  - CNN\n","  - Tensorflow/Keras\n","  - Grad-CAM\n","  - Interpretation\n","  - Activation Maps\n","format:\n","  html:\n","    code-fold: true\n","description: \"**Ziel**: Benutze und Interpretiere Deep Neural Networks\"\n","image: \"images/Sherlock.png\"\n","---"]},{"cell_type":"markdown","metadata":{},"source":["# Lernziele:\n","- Bildklassifizierung in der Praxis\n","- Interpretationsansätze (Grad-CAM)"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Lade Libraries"]},{"cell_type":"markdown","metadata":{},"source":["Im Folgenden benutzen wir die Tensorflow/Keras Software Plattform. Eine bekannte Alternative ist pyTorch.\n","\n","Beide Softwareloesungen kommen mit einem breiten Funktionsspektrum und ermöglichen auch die Benutzung von GPU.  \n","\n","GPU sind für das aufwendige Training grosser Neuraler Netzwerke unabdingbar. Hier werden wir nicht trainieren und können mit normalen CPU arbeiten."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND1lnm-PQeF_"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# convenience functions\n","def top_pred(preds, ntop=5):\n","\n","    # decode top predicitions (class_name and probabilities)\n","    decoded_predictions = decode_predictions(preds, top=ntop)[0]\n","\n","    # Extract class names, probabilities and indices\n","    class_names = [label[1] for label in decoded_predictions]\n","    class_probs = [label[2] for label in decoded_predictions]\n","    class_inds  = np.argsort(preds[0])[::-1][:ntop]\n","\n","    # define top class [id, name, prob] \n","    top_class = [class_inds[0], class_names[0], class_probs[0]]\n","\n","    fig, ax = plt.subplots()\n","    bars = ax.barh(class_inds.astype(str), class_probs)\n","    ax.set_xlabel('Class Probability')\n","    ax.set_ylabel('Class ID')\n","    ax.set_title('Top Predictions')\n","\n","    # Add class names\n","    for bar, id in zip(bars, class_names):\n","        ax.text(bar.get_width()/2, bar.get_y() + bar.get_height() / 2, f'{id}', va='center')\n","\n","    plt.show()\n","\n","    return top_class\n","\n","def plot_layer(x, model, layer, channels=(0,1,2)):\n","  output = model.layers[layer].output                          # define output from layer                     \n","  fmodel = tf.keras.Model(inputs=model.inputs, outputs=output) # model only up to layer\n","  amap   = fmodel.predict(x)                                   # activation map = feature map\n","\n","  print('layer: {} name: {} activation_map: {}'.format(layer, model.layers[layer].name, amap.shape))\n","  fig, ax = plt.subplots(nrows=1, ncols=len(channels), figsize=(12, 16))\n","\n","  ix = 0\n","  for c in channels:\n","\t  ax[ix].imshow(amap[0, :, :, c], cmap='gray')\n","\t  ix = ix + 1\n","  plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%script echo \"install tf-keras-vis if not yet in the environment\"\n","# not yet installed on google-colab\n","! pip install tf-keras-vis"]},{"cell_type":"markdown","metadata":{"id":"7LQp2ZFrg6dm"},"source":["# 2. Lade Model (optimiert)"]},{"cell_type":"markdown","metadata":{"id":"j0Qw5mIdtH46"},"source":["Ein klassisches Model ist **Vgg16**. Es wurde mit 14 Millionen Bildern ('ImageNet') aus 1000 Kategorien trainiert ('ImageNet') \n","\n","Computezeit: >3 Wochen mit 4 GPU!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4L3kdoswYtj"},"outputs":[],"source":["from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input, decode_predictions\n","model = VGG16()\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["![Vgg16. Illustration from paperswithcode.com](images/Vgg16.png)"]},{"cell_type":"markdown","metadata":{"id":"YUgrJmGYw3Xl"},"source":["Andere vortrainierte Modelle sind hier verfuegbar: [https://keras.io/api/applications/](https://keras.io/api/applications/) "]},{"cell_type":"markdown","metadata":{"id":"DFM2y66ut27U"},"source":["# 3. Lade Daten"]},{"cell_type":"markdown","metadata":{"id":"ivnnaG2ZXAct"},"source":["**Beachte**\n","\n","- Benutze Daten genau so wie vom Modell erwartet: reshape, convert, scale, normalize. $\\to$ Keras functions: *image.load_img*,  *preprocess_input*.\n","\n","- *decode_prediction* wird später helfen die labels (1-1000) in Kategorien zu übersetzen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ozpn6DZ8wrJd"},"outputs":[],"source":["# load more convenience functions\n","from keras.preprocessing import image\n","\n","\n","# choose your jpeg file\n","fn='images/elephant.jpg'\n","url='https://img-datasets.s3.amazonaws.com/elephant.jpg'\n","#fn=tf.keras.utils.get_file(origin=url)\n","\n","# determine target size of image\n","input_shape = model.input_shape                  # input shape is (,w,h,3)\n","input_size  = (input_shape[1], input_shape[2])   # expected size of input images (w x h)\n","print('input_size:',input_size)\n","\n","# load image (with proper size)\n","img = image.load_img(fn, target_size=input_size, keep_aspect_ratio=True)\n","plt.imshow(img)\n","plt.show()\n","\n","# data conversions and preprocessing\n","x = image.img_to_array(img)  # PIL -> numpy\n","x = x[np.newaxis, ...]       # expand to sample dimension, np.expand_dims(x, axis=0)\n","x = preprocess_input(x)      # preprocess as was done for Vgg16"]},{"cell_type":"markdown","metadata":{"id":"VZgrXkb6uAyr"},"source":["# 4. Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds = model.predict(x)\n","top_class = top_pred(preds)"]},{"cell_type":"markdown","metadata":{},"source":["## Einfach und schnell: Software + Model + Daten $\\to$ Predict"]},{"cell_type":"markdown","metadata":{},"source":["## Wie gut sind die Vorhersagen ?"]},{"cell_type":"markdown","metadata":{},"source":["![Accuracy on ImageNet: improvements of the bests. Source: https://paperswithcode.com/](images/ImageNet_acc.png)"]},{"cell_type":"markdown","metadata":{},"source":["Human performance can be tested here: [https://cs.stanford.edu/people/karpathy/ilsvrc/](https://cs.stanford.edu/people/karpathy/ilsvrc/)"]},{"cell_type":"markdown","metadata":{},"source":["## Woher kommt der Erfolg ?\n","\n","- Data Power: Big Data + Annotations\n","- Compute Power: GPU\n","- Brain Power: clever training algorithms\n","- Software Power: Standardisierung & Modularizierung (Legoprinzip: plug-n-play)\n","- Social Power: open source community, training materials, friendly competitions"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Interpretation"]},{"cell_type":"markdown","metadata":{"id":"Sy6edjjJywfn"},"source":["## 5.1 Visualize Outputs: Activation Maps"]},{"cell_type":"markdown","metadata":{},"source":["Hier schauen wir uns die Aktivierungsmuster der verschiedenen Ebenen im CNN an.\n","\n","Intuition:\n","\n","- Frühe Layers messen basale Strukturen und features (Linien etc.)\n","- Tiefere Layer \"erkennen\" Konzepte"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_layer(x, model, layer=1,channels=(1,2,63))\n","plot_layer(x, model, layer=17, channels=(1,2,511))"]},{"cell_type":"markdown","metadata":{"id":"I4Dju49J7Awn"},"source":["## 5.2 Summarizing Activation Maps"]},{"cell_type":"markdown","metadata":{"id":"Z1QgJGlha7Q_"},"source":["Welche Veränderungen in den Daten (X) haben den größten Einfluss auf die gemachte Vorhersage (class=\"African Elephant\")."]},{"cell_type":"markdown","metadata":{},"source":["![1 Frage - 2 Methoden: Welche Pixel sind für die Klassenvorhersage wichtig?](images/interpretation.jpg)"]},{"cell_type":"markdown","metadata":{},"source":["## 5.3 Anwendung"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"elapsed":6258,"status":"ok","timestamp":1655667226284,"user":{"displayName":"Thomas Manke","userId":"17591636328965298454"},"user_tz":-120},"id":"1uH1DvvlOFcS","outputId":"abaf6dee-d83b-4379-eab1-756e2014c680"},"outputs":[],"source":["from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n","from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n","from tf_keras_vis.utils.scores import CategoricalScore\n","\n","# get predicted label (class id)\n","top_class_id = top_class[0]\n","# get score of predicted label: y_c\n","score = CategoricalScore([top_class_id])\n","# gradcam object\n","gradcam = GradcamPlusPlus( model, model_modifier=ReplaceToLinear(), clone=True)\n","# 2D array of same size as x \\in [0,1]\n","cam     = gradcam(score, x) \n","# convert heatmap (RGB), ignore transparency, convert to int\n","heatmap = np.uint8(plt.cm.jet(cam[0])[..., :3] * 255)\n","\n","# plot and overlay\n","plt.figure(figsize=(12, 5))\n","ax = plt.subplot(1, 2, 1)\n","plt.imshow(img)\n","plt.title('Original')\n","\n","ax = plt.subplot(1, 2, 2)\n","plt.imshow(img)\n","plt.imshow(heatmap, cmap='jet', alpha=0.5) \n","plt.title('Grad-Cam')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VPx-ZuCIIwmQ"},"source":["## 5.4 Nutzen von Visualisierungen (Grad-CAM)"]},{"cell_type":"markdown","metadata":{"id":"6JcF_ufFI1GD"},"source":["- Interpretation von Vorhersagen (direct attention)\n","- Verstaendnis von Misklassifizierungen (e.g. bias detection)"]},{"cell_type":"markdown","metadata":{},"source":["![Grad-CAM on biased and unbiased networks](https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs11263-019-01228-7/MediaObjects/11263_2019_1228_Fig8_HTML.png)"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Outlook"]},{"cell_type":"markdown","metadata":{},"source":["- Bild Interpretation $\\to$ Modell Interpretation\n","    - maximal activation of neuron: Activation Maximation\n","    - occlusion sensitivity\n","- Quantifying Interpretabiltiy\n","    - synthetische Bilder mit und ohne features -> Vorhersage von score\n","    - Network dissection: Ueberlap von deep Neuronen und Konzepten\n","- Optimize Accuracy & Interpretierbarkeit !!!\n","- Not limited to CNN"]},{"cell_type":"markdown","metadata":{},"source":["# References\n","\n","- Vgg16: [Simonyan & Zisserman](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n","- Grad-CAM: [Selvaraju et al. (2019)](https://link.springer.com/article/10.1007/s11263-019-01228-7)\n","- Performance Evaluations: [https://paperswithcode.com/](https://paperswithcode.com/)\n","- Testing Human Performance: [https://cs.stanford.edu/people/karpathy/ilsvrc/](https://cs.stanford.edu/people/karpathy/ilsvrc/)\n","- Interpretation of Predictions: [Interpretable Machine Learning (2023)](https://christophm.github.io/interpretable-ml-book/index.html)\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["e3YOdijA-MjY"],"name":"ANN_007_GradCam.ipynb","provenance":[{"file_id":"https://github.com/abidlabs/deep-learning-genomics-primer/blob/master/A_Primer_on_Deep_Learning_in_Genomics_Public.ipynb","timestamp":1646747740505}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
